{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11381653,"sourceType":"datasetVersion","datasetId":7126566}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install mediapipe\nimport os\nimport glob\nimport torch\nimport torchaudio\nimport cv2\nimport numpy as np\nfrom torchvision.transforms import ToTensor\nfrom torchaudio.transforms import Resample\nfrom moviepy.editor import VideoFileClip\nfrom PIL import Image\nimport mediapipe as mp\n\nAUDIO_SR = 16000\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Setup MediaPipe face mesh\nmp_face_mesh = mp.solutions.face_mesh\nface_mesh = mp_face_mesh.FaceMesh(static_image_mode=False)\n\n# Extract lip region using MediaPipe Face Mesh\ndef extract_lip_region(frame):\n    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    results = face_mesh.process(img_rgb)\n\n    if not results.multi_face_landmarks:\n        return None\n\n    landmarks = results.multi_face_landmarks[0]\n    h, w, _ = frame.shape\n\n    # Lip landmark indices (based on MediaPipe documentation)\n    lip_indices = list(set([\n        61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291, 308,\n        78, 95, 88, 178, 87, 14, 317, 402, 318, 324\n    ]))\n\n    pts = [(int(landmark.x * w), int(landmark.y * h))\n           for i, landmark in enumerate(landmarks.landmark) if i in lip_indices]\n\n    x_coords, y_coords = zip(*pts)\n    x1, x2 = max(min(x_coords)-5, 0), min(max(x_coords)+5, w)\n    y1, y2 = max(min(y_coords)-5, 0), min(max(y_coords)+5, h)\n\n    cropped = frame[y1:y2, x1:x2]\n    if cropped.size == 0:\n        return None\n\n    cropped_resized = cv2.resize(cropped, (96, 96))\n    return ToTensor()(cropped_resized)\n\n# Extract video frames and lip regions\ndef extract_video_frames(video_path):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        lip = extract_lip_region(frame)\n        if lip is not None:\n            frames.append(lip)\n\n    cap.release()\n    return torch.stack(frames) if frames else None\n\n# Extract audio features using Wav2Vec2\ndef extract_audio_tokens(video_path):\n    temp_audio = \"temp_audio.wav\"\n    clip = VideoFileClip(video_path)\n    clip.audio.write_audiofile(temp_audio, fps=AUDIO_SR, verbose=False, logger=None)\n    waveform, sr = torchaudio.load(temp_audio)\n    if sr != AUDIO_SR:\n        waveform = Resample(sr, AUDIO_SR)(waveform)\n\n    with torch.inference_mode():\n        model = torchaudio.pipelines.WAV2VEC2_BASE.get_model()\n        features, _ = model.extract_features(waveform[0].unsqueeze(0))\n\n    return features[0].squeeze(0)  # [T, D]\n\n# Process single sample\ndef process_sample(folder, vid_id):\n    video_path = os.path.join(folder, f\"{vid_id}.mp4\")\n    txt_path = os.path.join(folder, f\"{vid_id}.txt\")\n\n    if not os.path.exists(video_path) or not os.path.exists(txt_path):\n        return None\n\n    with open(txt_path, 'r') as f:\n        label = f.read().strip()\n\n    video = extract_video_frames(video_path)\n    if video is None:\n        print(f\"Skipping {vid_id}: no valid lip region.\")\n        return None\n\n    audio = extract_audio_tokens(video_path)\n    return {\"video\": video, \"audio\": audio, \"label\": label}\n\n# Process entire folder of videos\ndef process_folder(folder_path, save_path):\n    os.makedirs(save_path, exist_ok=True)\n    files = glob.glob(os.path.join(folder_path, \"*.mp4\"))\n    for vid_file in files:\n        vid_id = os.path.basename(vid_file).split(\".\")[0]\n        data = process_sample(folder_path, vid_id)\n        if data:\n            torch.save(data, os.path.join(save_path, f\"{vid_id}.pt\"))","metadata":{"_uuid":"f6a055d9-8028-4270-a4e6-269adbec90ad","_cell_guid":"2952418a-93ca-468e-bf8a-90ad71740deb","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-04T13:36:40.180809Z","iopub.execute_input":"2025-05-04T13:36:40.181325Z","iopub.status.idle":"2025-05-04T13:37:23.229406Z","shell.execute_reply.started":"2025-05-04T13:36:40.181292Z","shell.execute_reply":"2025-05-04T13:37:23.228086Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main_root = \"/kaggle/input/lets-do-it-once-and-for-all-dm-10-gb/main_subset_10gb\"  # ‚Üê Replace with actual path, e.g. \"/kaggle/input/lrs2/main\"\n    save_root = \"preprocessed/lrs2_subset_all2\"\n\n    subfolders = sorted([\n        os.path.join(main_root, name)\n        for name in os.listdir(main_root)\n        if os.path.isdir(os.path.join(main_root, name))\n    ])\n    counter = 1\n    for subfolder in subfolders:\n        \n            \n        folder_name = os.path.basename(subfolder)\n        counter+=1\n        print(counter,end=\" \")\n        if counter%40==1:\n            print(f\"\\nüì¶ Processing folder: {folder_name}\")\n            #break\n        process_folder(\n            folder_path=subfolder,\n            save_path=os.path.join(save_root, folder_name)\n        )","metadata":{"_uuid":"b382ea83-5a97-40da-a21f-968986f32eca","_cell_guid":"155a922e-2981-4296-ac84-f1b158754165","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-04T13:37:29.917882Z","iopub.execute_input":"2025-05-04T13:37:29.919256Z","iopub.status.idle":"2025-05-04T13:38:07.322180Z","shell.execute_reply.started":"2025-05-04T13:37:29.919222Z","shell.execute_reply":"2025-05-04T13:38:07.320663Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"dd9a9ed7-4d8a-48ae-9d35-3e442dd3b3cf","_cell_guid":"f997fd3b-208d-4efe-a9d6-06c52c55a314","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-04T13:40:12.556476Z","iopub.execute_input":"2025-05-04T13:40:12.556914Z","iopub.status.idle":"2025-05-04T13:40:12.655828Z","shell.execute_reply.started":"2025-05-04T13:40:12.556876Z","shell.execute_reply":"2025-05-04T13:40:12.654768Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}