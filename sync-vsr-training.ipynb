{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11675265,"sourceType":"datasetVersion","datasetId":7327534},{"sourceId":388448,"sourceType":"modelInstanceVersion","modelInstanceId":320117,"modelId":340666}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, glob\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader","metadata":{"_uuid":"256ca28d-055a-4e99-b8d6-f4d984dc3bd1","_cell_guid":"b1331b00-7f73-4ff1-8aea-743a1d82e900","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-14T14:05:00.639297Z","iopub.execute_input":"2025-05-14T14:05:00.639669Z","iopub.status.idle":"2025-05-14T14:05:05.922951Z","shell.execute_reply.started":"2025-05-14T14:05:00.639635Z","shell.execute_reply":"2025-05-14T14:05:05.921956Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CharTokenizer:\n    def __init__(self):\n        chars = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ '\")  # LRS2 has capital letters and space\n        self.stoi = {ch: i+1 for i, ch in enumerate(chars)}  # Reserve 0 for padding\n        self.itos = {i+1: ch for ch, i in self.stoi.items()}\n        self.pad_id = 0\n\n    def encode(self, text):\n        text = text.upper()\n        return [self.stoi.get(c, self.pad_id) for c in text if c in self.stoi]\n\n    def decode(self, indices):\n        return ''.join([self.itos.get(i, '') for i in indices if i > 0])","metadata":{"_uuid":"1d0b401e-a458-493b-8656-4a8c2d8bbf3d","_cell_guid":"b87a6e5f-2c38-4b99-908e-7e59a22269a3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-14T14:05:05.925055Z","iopub.execute_input":"2025-05-14T14:05:05.925571Z","iopub.status.idle":"2025-05-14T14:05:05.933425Z","shell.execute_reply.started":"2025-05-14T14:05:05.925535Z","shell.execute_reply":"2025-05-14T14:05:05.932218Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport re\n\n# class LRS2PTDataset(Dataset):\n#     def __init__(self, pt_paths, tokenizer):\n#         self.pt_paths = pt_paths\n#         self.tokenizer = tokenizer\n\n#     def __len__(self):\n#         return len(self.pt_paths)\n\n#     def __getitem__(self, idx):\n#         try:\n#             data = torch.load(self.pt_paths[idx])\n#             video = data['video']\n#             audio = data['audio']\n#             match = re.search(r'Text:\\s*(.*?)\\n', data['label'])\n#             if not match:\n#                 raise ValueError(\"Invalid label format\")\n#             text = match.group(1).strip()\n#             label = torch.tensor(self.tokenizer.encode(text), dtype=torch.long)\n#             return video, audio, label\n#         except Exception as e:\n#             print(f\"Skipping file {self.pt_paths[idx]} due to error: {e}\")\n#             return self.__getitem__((idx + 1) % len(self.pt_paths))\nimport torch\nimport re\n\nclass LRS2PTDataset(Dataset):\n    def __init__(self, pt_paths, tokenizer):\n        self.pt_paths = pt_paths\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.pt_paths)\n\n    def __getitem__(self, idx):\n        try:\n            # Load .pt file\n            data = torch.load(self.pt_paths[idx])\n            video = data['video']  # (T_v, 1, 96, 96)\n            audio = data['audio']\n            # Extract text label\n            match = re.search(r'Text:\\s*(.*?)\\n', data['label'])\n            if not match:\n                print(f\"Skipping {self.pt_paths[idx]}: invalid label format\")\n                return self.__getitem__((idx + 1) % len(self.pt_paths))\n            \n            text = match.group(1).strip()\n            # Skip samples with digits\n            if any(c.isdigit() for c in text):\n                #print(f\"Skipping {self.pt_paths[idx]}: contains digits in text: {text}\")\n                #print()\n                return self.__getitem__((idx + 1) % len(self.pt_paths))\n            \n            # Encode text\n            label = self.tokenizer.encode(text)\n            video_length = video.shape[0]\n            # Truncate label to video length\n            label = label[:video_length]\n            if len(label) == 0:\n                print(f\"Skipping {self.pt_paths[idx]}: empty label after encoding/truncation\")\n                return self.__getitem__((idx + 1) % len(self.pt_paths))\n            \n            # Validate tokens\n            vocab_size = len(self.tokenizer.stoi) + 1  # 28 (1-27 + blank)\n            if any(token >= vocab_size or token < 0 for token in label):\n                print(f\"Skipping {self.pt_paths[idx]}: invalid tokens in label: {label}\")\n                return self.__getitem__((idx + 1) % len(self.pt_paths))\n            \n            label = torch.tensor(label, dtype=torch.long)\n            return video, audio, label\n        \n        except Exception as e:\n            print(f\"Skipping {self.pt_paths[idx]}: error loading file: {e}\")\n            return self.__getitem__((idx + 1) % len(self.pt_paths))\n\n# def collate_fn(batch):\n#     videos, audios, labels = zip(*batch)\n#     max_len_v = max(v.shape[0] for v in videos)\n#     max_len_a = max(a.shape[0] for a in audios)\n    \n\n#     def pad(x, target_len):\n#         if x.shape[0] < target_len:\n#             pad_size = (0, 0) * (x.dim() - 1) + (0, target_len - x.shape[0])\n#             return torch.nn.functional.pad(x, pad_size)\n#         return x\n#     # for i in [pad(a, max_len_a) for a in audios]:\n#     #     print(i.shape)\n\n#     # print(\"For videos\")\n#     # for i in [pad(v, max_len_v) for v in videos]:\n#     #     print(i.shape)\n#     padded_videos = torch.stack([pad(v, max_len_v) for v in videos])  # (B, T, 1, 96, 96)\n#     #padded_videos = padded_videos.permute(0, 2, 1, 3, 4)  # (B, 1, T, 96, 96)\n#     padded_audios = torch.stack([pad(a, max_len_a) for a in audios])  # (B, T, 768)\n#     padded_labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=0)\n#     label_lengths = torch.tensor([len(l) for l in labels])\n    \n#     #print(\"returning\")\n#     #print(padded_videos.shape)\n#     return padded_videos, padded_audios, padded_labels, label_lengths\ndef collate_fn(batch):\n    videos, audios, labels = zip(*batch)\n    max_len_v = max(v.shape[0] for v in videos)  # Maximum video length\n    max_len_a = max(a.shape[0] for a in audios)  # Maximum audio length\n    max_len_labels = max(len(l) for l in labels)  # Maximum label length\n\n    # Pad videos to at least max_len_labels to satisfy CTC requirement\n    target_video_len = max(max_len_v, max_len_labels)\n\n    def pad(x, target_len):\n        if x.shape[0] < target_len:\n            pad_size = (0, 0) * (x.dim() - 1) + (0, target_len - x.shape[0])\n            return torch.nn.functional.pad(x, pad_size)\n        return x\n\n    video_lengths = [v.shape[0] for v in videos]  # Original video lengths\n    # Convert 1-channel to 3-channel if needed\n    videos = [v.repeat(1, 3, 1, 1) if v.shape[1] == 1 else v for v in videos]  # (T, 3, 96, 96)\n    # Pad videos to target_video_len\n    padded_videos = torch.stack([pad(v, target_video_len) for v in videos])  # (B, T, 3, 96, 96)\n    padded_audios = torch.stack([pad(a, max_len_a) for a in audios])  # (B, T, 768)\n    padded_labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=0)\n    label_lengths = torch.tensor([len(l) for l in labels])\n    # Adjust video_lengths to reflect padding\n    video_lengths = torch.tensor([min(vl, target_video_len) for vl in video_lengths], dtype=torch.long)\n\n    return padded_videos, padded_audios, padded_labels, label_lengths\n\nfrom torch.utils.data import Sampler\nimport random\n#prompt of bucket sort already given\nclass BucketBatchSampler(Sampler):\n    def __init__(self, lengths, batch_size, drop_last=False):\n        self.batch_size = batch_size\n        self.drop_last = drop_last\n\n        # Sort indices by length\n        self.sorted_indices = sorted(range(len(lengths)), key=lambda i: lengths[i])\n        self.batches = [\n            self.sorted_indices[i:i+batch_size]\n            for i in range(0, len(self.sorted_indices), batch_size)\n        ]\n        random.shuffle(self.batches)\n\n    def __iter__(self):\n        for batch in self.batches:\n            yield batch\n\n    def __len__(self):\n        return len(self.batches)","metadata":{"_uuid":"9c9e2bf4-9fba-49a9-abef-7d75a4d457d4","_cell_guid":"798c557b-d492-4fda-a672-0dec65a34c16","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-14T14:05:05.934731Z","iopub.execute_input":"2025-05-14T14:05:05.935128Z","iopub.status.idle":"2025-05-14T14:05:05.960649Z","shell.execute_reply.started":"2025-05-14T14:05:05.935095Z","shell.execute_reply":"2025-05-14T14:05:05.959482Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision.models import resnet18\nfrom einops import rearrange\n\nimport torch\nimport torch.nn as nn\nfrom torchvision.models import resnet18\nfrom einops import rearrange\nimport torch\nimport torch.nn as nn\nfrom torchvision.models import resnet34\nfrom einops import rearrange\n\nclass TimeDistributed(nn.Module):\n    def __init__(self, module):\n        super().__init__()\n        self.module = module\n\n    def forward(self, x):\n        # Input: (B, T, C, H, W)\n        B, T, C, H, W = x.shape\n        x = rearrange(x, 'b t c h w -> (b t) c h w')  # (B*T, C, H, W)\n        x = self.module(x)  # Apply module to each frame\n        x = rearrange(x, '(b t) c h w -> b t c h w', b=B, t=T)  # (B, T, C', H', W')\n        return x\n\nclass VisualEncoder(nn.Module):\n    def __init__(self, embed_dim=512):\n        super().__init__()\n        # Enhanced 3D CNN backbone with additional layers\n        self.cnn3d = nn.Sequential(\n            nn.Conv3d(3, 64, (3, 5, 5), (1, 2, 2), (1, 2, 2)),  # (B, 3, T, 96, 96) -> (B, 64, T, 24, 24)\n            nn.BatchNorm3d(64),  # Stabilize activations\n            nn.ReLU(),\n            nn.Conv3d(64, 128, (3, 3, 3), padding=1),  # (B, 128, T, 24, 24)\n            nn.BatchNorm3d(128),\n            nn.ReLU(),\n            nn.MaxPool3d((1, 2, 2)),  # (B, 128, T, 12, 12)\n            nn.Conv3d(128, 256, (3, 3, 3), padding=1),  # (B, 256, T, 12, 12)\n            nn.BatchNorm3d(256),\n            nn.ReLU(),\n            # Additional 3D CNN layer for deeper spatiotemporal features\n            nn.Conv3d(256, 256, (3, 3, 3), padding=1),  # (B, 256, T, 12, 12)\n            nn.BatchNorm3d(256),\n            nn.ReLU(),\n            nn.MaxPool3d((1, 2, 2)),  # (B, 256, T, 6, 6)\n        )\n\n        # TimeDistributed 2D CNN for per-frame feature enhancement\n        self.timedistributed = TimeDistributed(\n            nn.Sequential(\n                nn.Conv2d(256, 128, kernel_size=3, padding=1),  # (B*T, 256, 6, 6) -> (B*T, 128, 6, 6)\n                nn.BatchNorm2d(128),\n                nn.ReLU(),\n                nn.Conv2d(128, 128, kernel_size=3, padding=1),  # (B*T, 128, 6, 6)\n                nn.BatchNorm2d(128),\n                nn.ReLU(),\n            )\n        )\n\n        # Upgrade to ResNet-34 for better feature extraction\n        resnet = resnet34(pretrained=False)\n        resnet.conv1 = nn.Conv2d(128, 64, kernel_size=7, stride=2, padding=3, bias=False)  # Adjusted for input channels\n        self.resnet = nn.Sequential(*list(resnet.children())[:-2])  # Remove avgpool and fc\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512, embed_dim)\n\n        # Initialize weights to stabilize training\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        # Initialize Conv3d and Conv2d layers with Kaiming normal\n        for m in self.modules():\n            if isinstance(m, (nn.Conv3d, nn.Conv2d)):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm3d) or isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):  # Input: (B, T, 3, 96, 96)\n        B, T = x.shape[0], x.shape[1]\n        x = x.permute(0, 2, 1, 3, 4)  # (B, 3, T, 96, 96)\n        x = self.cnn3d(x)  # (B, 256, T, 6, 6)\n        x = rearrange(x, 'b c t h w -> b t c h w')  # (B, T, 256, 6, 6)\n        x = self.timedistributed(x)  # (B, T, 128, 6, 6)\n        x = rearrange(x, 'b t c h w -> (b t) c h w')  # (B*T, 128, 6, 6)\n        x = self.resnet(x)  # (B*T, 512, H', W')\n        x = self.pool(x).squeeze(-1).squeeze(-1)  # (B*T, 512)\n        x = rearrange(x, '(b t) c -> b t c', b=B, t=T)  # (B, T, 512)\n        return self.fc(x)  # (B, T, 512)\n\n# class VisualEncoder(nn.Module):\n#     def __init__(self, embed_dim=512):\n#         super().__init__()\n#         self.cnn3d = nn.Sequential(\n#             nn.Conv3d(3, 64, (3, 5, 5), (1, 2, 2), (1, 2, 2)),  # (B, 3, T, 96, 96) -> (B, 64, T, 24, 24)\n#             nn.ReLU(),\n#             nn.Conv3d(64, 128, (3, 3, 3), padding=1),           # (B, 128, T, 24, 24)\n#             nn.ReLU(),\n#             nn.MaxPool3d((1, 2, 2)),                            # (B, 128, T, 12, 12)\n#             nn.Conv3d(128, 256, (3, 3, 3), padding=1),\n#             nn.ReLU(),\n#         )\n        \n#         # Feeding into ResNet\n#         resnet = resnet18(pretrained=False)\n#         resnet.conv1 = nn.Conv2d(256, 64, kernel_size=7, stride=2, padding=3, bias=False)\n#         self.resnet = nn.Sequential(*list(resnet.children())[:-2])  # No avgpool/fc\n#         self.pool = nn.AdaptiveAvgPool2d((1, 1))\n#         self.fc = nn.Linear(512, embed_dim)\n\n#     def forward(self, x):  # (B, T, 3, 96, 96)\n#         B, T = x.shape[0], x.shape[1]\n#         x = x.permute(0, 2, 1, 3, 4)       # (B, 3, T, 96, 96)\n#         x = self.cnn3d(x)                  # (B, 256, T, H, W)\n#         x = rearrange(x, 'b c t h w -> (b t) c h w')\n#         x = self.resnet(x)                # (B*T, 512, H', W')\n#         x = self.pool(x).squeeze(-1).squeeze(-1)  # (B*T, 512)\n#         x = rearrange(x, '(b t) c -> b t c', b=B, t=T)\n#         return self.fc(x)  # (B, T, 512)","metadata":{"_uuid":"1cd8e805-bf86-49e4-a6dd-fcb9d568784c","_cell_guid":"1eca5826-f0dd-4220-ac48-2971cbea7908","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-14T14:05:05.961678Z","iopub.execute_input":"2025-05-14T14:05:05.962027Z","iopub.status.idle":"2025-05-14T14:05:10.004163Z","shell.execute_reply.started":"2025-05-14T14:05:05.961988Z","shell.execute_reply":"2025-05-14T14:05:10.003162Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch.nn as nn\n\n# class SyncVSR(nn.Module):\n#     def __init__(self, visual_encoder, vocab_size, audio_dim=768):\n#         super().__init__()\n#         self.visual_encoder = visual_encoder\n#         self.temporal = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=512, nhead=8), num_layers=4)\n#         self.ctc_head = nn.Linear(512, vocab_size)\n#         self.sync_head = nn.Linear(512, audio_dim)\n#         # print(\"initialized\")\n\n#     def forward(self, video):\n#         x = self.visual_encoder(video)          # (B, T, 512)\n#         x = self.temporal(x.permute(1, 0, 2))   # (T, B, 512)\n#         x = x.permute(1, 0, 2)                  # (B, T, 512)\n#         ctc_logits = self.ctc_head(x)\n#         audio_pred = self.sync_head(x)\n#         return ctc_logits, audio_pred\n# #\"The problem is here we are not receiving (B, T, 512) but rather torch.Size([B, 1, T, 96, 96])\"\nclass SyncVSR(nn.Module):\n    def __init__(self, visual_encoder, vocab_size, audio_dim=768):\n        super().__init__()\n        self.visual_encoder = visual_encoder\n        self.pos_encoding = nn.Parameter(torch.randn(500, 512))  # T_max = 500\n        encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, dropout=0.1)\n        self.temporal = nn.TransformerEncoder(encoder_layer, num_layers=6)\n        self.norm = nn.LayerNorm(512)\n        self.ctc_head = nn.Linear(512, vocab_size)\n        self.sync_head = nn.Linear(512, audio_dim)\n\n    def forward(self, video):  # (B, T, 3, 96, 96)\n        x = self.visual_encoder(video)       # (B, T, 512)\n        T = x.size(1)\n        x = x + self.pos_encoding[:T]        # Add temporal pos encoding\n        x = self.temporal(x.permute(1, 0, 2))  # (T, B, 512)\n        x = self.norm(x).permute(1, 0, 2)      # (B, T, 512)\n        ctc_logits = self.ctc_head(x)         # (B, T, vocab_size)\n        audio_pred = self.sync_head(x)        # (B, T, audio_dim)\n        return ctc_logits, audio_pred","metadata":{"_uuid":"a1f2f242-459e-4c5d-b086-77afc956e0b3","_cell_guid":"2eec099d-60e9-4b67-8ef8-9fca50f132bd","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-14T14:05:10.006780Z","iopub.execute_input":"2025-05-14T14:05:10.007216Z","iopub.status.idle":"2025-05-14T14:05:10.016110Z","shell.execute_reply.started":"2025-05-14T14:05:10.007188Z","shell.execute_reply":"2025-05-14T14:05:10.014712Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Load data\ntokenizer = CharTokenizer()\npt_paths = glob.glob('/kaggle/input/sync-vsr-better/preprocessed/lrs2_subset_all2/*/*.pt')\ndataset = LRS2PTDataset(pt_paths, tokenizer)\n\n# Get video lengths (number of frames)\nvideo_lengths = []\nfor path in dataset.pt_paths:\n    try:\n        data = torch.load(path, map_location=\"cpu\")\n        video_lengths.append(data['video'].shape[0])  # T (frames)\n    except:\n        video_lengths.append(0)  # Just to avoid crashing\n\nsampler = BucketBatchSampler(video_lengths, batch_size=4)\nloader = DataLoader(dataset, batch_sampler=sampler, collate_fn=collate_fn)\n\nprint(\"wow\")\n# Build model\nmodel = SyncVSR(VisualEncoder(), vocab_size=len(tokenizer.stoi)+1).to(device)\nprint(\"Entered training\")\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\nlambda_sync = 1.0\ncounter = 0","metadata":{"_uuid":"54ab4717-6ce4-4923-875b-8c03e77b3dcb","_cell_guid":"e36d22e3-688d-4dfe-abc0-a9d033c98b82","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-14T14:05:10.017191Z","iopub.execute_input":"2025-05-14T14:05:10.017657Z","iopub.status.idle":"2025-05-14T14:08:06.139496Z","shell.execute_reply.started":"2025-05-14T14:05:10.017627Z","shell.execute_reply":"2025-05-14T14:08:06.138410Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import glob\nimport json\nimport os\n\n# Directory for saving checkpoints\ncheckpoint_dir = '/kaggle/input/checkpoint-syncvsr/pytorch/default/1/checkpoints'\nos.makedirs(checkpoint_dir, exist_ok=True)\n\n# Initialize start epoch\nstart_epoch = 0\n\n# Find all metadata files and get the latest\nmetadata_files = glob.glob(os.path.join(checkpoint_dir, 'metadata_epoch_*.json'))\nif metadata_files:\n    latest_metadata_file = max(metadata_files, key=os.path.getctime)\n    \n    # Extract epoch number from filename\n    basename = os.path.basename(latest_metadata_file)\n    epoch_num = int(basename.split('_')[-1].split('.')[0])\n\n    # Build file paths\n    model_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch_num}.pt')\n    optimizer_path = os.path.join(checkpoint_dir, f'optimizer_epoch_{epoch_num}.pt')\n\n    # Load model and optimizer state dicts\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    optimizer.load_state_dict(torch.load(optimizer_path, map_location=device))\n\n    # Load metadata\n    with open(latest_metadata_file, 'r') as f:\n        metadata = json.load(f)\n        start_epoch = metadata['epoch']\n    \n    print(f\"✅ Loaded checkpoint: Epoch {start_epoch}\")\nelse:\n    print(\"⚠️ No checkpoint found. Starting fresh.\")","metadata":{"_uuid":"4eb46104-c698-48e9-af61-96703a8136a9","_cell_guid":"70e7047b-e92a-45ca-91fc-91150867ae16","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-14T14:08:06.140556Z","iopub.execute_input":"2025-05-14T14:08:06.140863Z","iopub.status.idle":"2025-05-14T14:08:09.599471Z","shell.execute_reply.started":"2025-05-14T14:08:06.140839Z","shell.execute_reply":"2025-05-14T14:08:09.598131Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfor epoch in range(50):\n    print(\"Entered training\")\n    model.train()\n    print(\"training\")\n    total_loss = 0\n    vocab_size = len(tokenizer.stoi)+1\n    for batch in loader:\n        video, audio, labels, label_lens = [x.to(device) for x in batch]\n        #video = video.permute(0, 2, 1, 3, 4)  # (B, 1, T, H, W)\n        #print(\"Videodm\")\n        # AT this point torch.Size([8, 107, 1, 96, 96]) becomes torch.Size([8, 1, 107, 96, 96])\n        ctc_logits, audio_preds = model(video)  # (B, T, Vocab), (B, T, 768)\n        input_lengths = torch.full((video.size(0),), ctc_logits.size(1), dtype=torch.long).to(device)\n        min_len = min(audio_preds.size(1), audio.size(1))\n        max_len = max(audio_preds.size(1), audio.size(1))\n        if (input_lengths < label_lens).any():\n            print(f\"input_lengths: {input_lengths},\\t label_lens: {label_lens}\")\n            print(\"💥 Skipping batch due to input < label length\")\n            print(f\"video.size(0): {video.size(0)}\")\n            print(f\"Video shape: {video.shape}\")\n            print(f\"CTC logits shape: {ctc_logits.shape}\")\n            print(f\"Input lengths: {input_lengths}\")\n            print(f\"Label lengths: {label_lens}\")\n            print(f\"Label shape: {labels.shape}\")\n            continue\n        for i, (il, ll) in enumerate(zip(input_lengths, label_lens)):\n            if il < ll:\n                print(f\"💥 Sample {i}: input length {il.item()} < label length {ll.item()}\")\n        if audio_preds.size(1) < audio.size(1):\n            pad_size = audio.size(1) - audio_preds.size(1)\n            audio_preds = F.pad(audio_preds, (0, 0, 0, pad_size))  # pad along time dimension\n            #print(f\"Padding audio_preds to length {audio.size(1)}\")\n        elif audio_preds.size(1) > audio.size(1):\n            audio_preds = audio_preds[:, :audio.size(1), :]\n            #print(f\"Cropping audio_preds to length {audio.size(1)}\")\n        # Losses\n        if torch.any(torch.isnan(ctc_logits)) or torch.any(torch.isinf(ctc_logits)):\n            print(\"CTC logits contain NaN or Inf!\")\n        if (input_lengths < label_lens).any():\n            print(\"Input lengths are shorter than label lengths! That's a CTC death sentence.\")\n        if torch.any(labels >= vocab_size):  # replace vocab_size with your actual size\n            print(\"Label has index out of vocab bounds!\")\n            \n        ctc = F.ctc_loss(ctc_logits.log_softmax(2).transpose(0, 1), labels, input_lengths, label_lens, blank=0)\n        sync = F.mse_loss(audio_preds, audio)\n        loss = ctc + lambda_sync * sync\n        if torch.isnan(loss) or torch.isinf(loss):\n            #print(\"🔥 Skipping batch with corrupted loss!\")\n            #print(f\"loss = {ctc} + {lambda_sync} * {sync}\")\n            print()\n            #print(f\"Video shape: {video.shape}\")\n            #print(f\"Audio shape: {audio.shape}\")\n            #print(f\"Labels: {labels}\")\n           # print(f\"Label lengths: {label_lens}\")\n            #print(f\"Input lengths: {input_lengths}\")\n            #print(f\"CTC logits stats: min={ctc_logits.min().item()}, max={ctc_logits.max().item()}\")\n            continue\n        if counter%100 == 0:\n            print(f\"loss = {ctc} + {lambda_sync} * {sync}\")\n            #print(\"🔥 Skipping batch with corrupted loss!\")\n            print(f\"loss = {ctc} + {lambda_sync} * {sync}\")\n            print(f\"Video shape: {video.shape}\")\n            print(f\"Audio shape: {audio.shape}\")\n            #print(f\"Labels: {labels}\")\n            print(f\"Label lengths: {label_lens}\")\n            print(f\"Input lengths: {input_lengths}\")\n            print(f\"CTC logits stats: min={ctc_logits.min().item()}, max={ctc_logits.max().item()}\")\n            \n        counter+=1\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n        optimizer.step()\n\n        total_loss += loss.item()\n    \n\n    if (epoch + 1) % 10 == 0:\n        model_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch+1}.pt')\n        optimizer_path = os.path.join(checkpoint_dir, f'optimizer_epoch_{epoch+1}.pt')\n        metadata_path = os.path.join(checkpoint_dir, f'metadata_epoch_{epoch+1}.json')\n    \n        # Save model and optimizer separately\n        torch.save(model.state_dict(), model_path)\n        torch.save(optimizer.state_dict(), optimizer_path)\n    \n        # Save metadata in a readable JSON format\n        metadata = {\n            'epoch': epoch + 1,\n            'loss': total_loss / len(loader)\n        }\n        with open(metadata_path, 'w') as f:\n            json.dump(metadata, f, indent=4)\n    \n        print(f\"Saved model: {model_path}\")\n        print(f\"Saved optimizer: {optimizer_path}\")\n        print(f\"Saved metadata: {metadata_path}\")\n\n        \n    print(f\"[Epoch {epoch+1}] Loss: {total_loss / len(loader):.4f}\")","metadata":{"_uuid":"acaf1910-0246-43f7-9b01-71b351ae1e36","_cell_guid":"9b9c98b4-b31d-43cc-a10a-d2a4ff770098","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-14T14:08:09.600483Z","iopub.execute_input":"2025-05-14T14:08:09.600777Z","execution_failed":"2025-05-14T17:27:13.655Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import json\n# import random\n# import torch\n# import torch.nn.functional as F\n# from torch.cuda.amp import GradScaler, autocast\n# from torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# def greedy_ctc_decode(logits, tokenizer):\n#     \"\"\"Greedy CTC decoding: select the most likely token at each time step.\"\"\"\n#     # logits: (T, vocab_size)\n#     predicted_ids = torch.argmax(logits, dim=-1)  # (T,)\n#     # Collapse repeated tokens and remove blanks (0)\n#     output = []\n#     prev_id = None\n#     for id in predicted_ids:\n#         if id != 0 and id != prev_id:  # Skip blanks and repeats\n#             output.append(id.item())\n#         prev_id = id\n#     return tokenizer.decode(output)\n\n# def run_inference(model, dataset, tokenizer, device, sample_idx=None):\n#     \"\"\"Run inference on a single sample and return predicted/ground truth text.\"\"\"\n#     model.eval()\n#     with torch.no_grad():\n#         # Select a random sample if no index is provided\n#         idx = sample_idx if sample_idx is not None else random.randint(0, len(dataset) - 1)\n#         try:\n#             video, _, label = dataset[idx]  # Only need video and label for inference\n#             video = video.unsqueeze(0).to(device)  # (1, T, 3, 96, 96)\n#             ctc_logits, _ = model(video)  # (1, T, vocab_size)\n#             ctc_logits = ctc_logits.squeeze(0)  # (T, vocab_size)\n            \n#             # Decode prediction\n#             pred_text = greedy_ctc_decode(ctc_logits, tokenizer)\n#             # Ground truth\n#             gt_text = tokenizer.decode(label.tolist())\n#             return pred_text, gt_text, idx\n#         except Exception as e:\n#             print(f\"Inference failed for sample {idx}: {e}\")\n#             return None, None, idx\n\n# # Training setup (assumes prior code for dataset, model, etc.)\n# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n# scaler = GradScaler()\n# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n\n# # Fixed sample for consistent inference (optional, set to None for random)\n# fixed_inference_idx = None  # Change to an index (e.g., 0) for a specific sample\n\n# for epoch in range(start_epoch, 50):\n#     print(f\"Epoch {epoch+1} started\")\n#     model.train()\n#     total_loss = 0\n#     vocab_size = len(tokenizer.stoi) + 1\n#     for batch_idx, batch in enumerate(loader):\n#         video, audio, labels, label_lens = [x.to(device) for x in batch]\n#         input_lengths = torch.full((video.size(0),), video.size(1), dtype=torch.long).to(device)\n\n#         optimizer.zero_grad(set_to_none=True)\n#         with autocast():\n#             ctc_logits, audio_preds = model(video)\n#             if torch.any(torch.isnan(ctc_logits)) or torch.any(torch.isinf(ctc_logits)):\n#                 print(f\"Batch {batch_idx}: NaN/Inf in CTC logits\")\n#                 continue\n\n#             # Adjust audio lengths\n#             min_len = min(audio_preds.size(1), audio.size(1))\n#             audio_preds = audio_preds[:, :min_len, :]\n#             audio = audio[:, :min_len, :]\n\n#             ctc = F.ctc_loss(ctc_logits.log_softmax(2).transpose(0, 1), labels, input_lengths, label_lens, blank=0)\n#             sync = F.mse_loss(audio_preds, audio)\n#             loss = ctc + lambda_sync * sync\n\n#         if torch.isnan(loss) or torch.isinf(loss):\n#             print(f\"Batch {batch_idx}: NaN/Inf Loss: ctc={ctc.item()}, sync={sync.item()}\")\n#             continue\n\n#         scaler.scale(loss).backward()\n#         scaler.unscale_(optimizer)\n#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n#         scaler.step(optimizer)\n#         scaler.update()\n\n#         total_loss += loss.item()\n#         if batch_idx % 100 == 0:\n#             print(f\"Batch {batch_idx}: Loss = {loss.item():.4f} (CTC: {ctc.item():.4f}, Sync: {sync.item():.4f})\")\n#             print(f\"Video shape: {video.shape}, Label lengths: {label_lens}, Input lengths: {input_lengths}\")\n\n#     avg_loss = total_loss / len(loader)\n#     scheduler.step(avg_loss)\n\n#     # Inference after each epoch\n#     pred_text, gt_text, sample_idx = run_inference(model, dataset, tokenizer, device, fixed_inference_idx)\n#     if pred_text is not None:\n#         print(f\"\\n[Inference after Epoch {epoch+1}] Sample {sample_idx}\")\n#         print(f\"Predicted: {pred_text}\")\n#         print(f\"Ground Truth: {gt_text}\\n\")\n#     else:\n#         print(f\"\\n[Inference after Epoch {epoch+1}] Failed for sample {sample_idx}\\n\")\n\n#     # Save checkpoints every 5 epochs\n#     if (epoch + 1) % 10 == 0:\n#         model_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch+1}.pt')\n#         optimizer_path = os.path.join(checkpoint_dir, f'optimizer_epoch_{epoch+1}.pt')\n#         metadata_path = os.path.join(checkpoint_dir, f'metadata_epoch_{epoch+1}.json')\n        \n#         torch.save(model.state_dict(), model_path)\n#         torch.save(optimizer.state_dict(), optimizer_path)\n#         metadata = {'epoch': epoch + 1, 'loss': avg_loss}\n#         with open(metadata_path, 'w') as f:\n#             json.dump(metadata, f, indent=4)\n        \n#         print(f\"Saved: {model_path}, {optimizer_path}, {metadata_path}\")\n\n#     print(f\"[Epoch {epoch+1}] Loss: {avg_loss:.4f}\")","metadata":{"_uuid":"18d237c9-417e-4cee-861a-dd74d129d34e","_cell_guid":"ea4baf01-fb42-4e70-8a2f-b0f55905f117","trusted":true,"collapsed":false,"execution":{"execution_failed":"2025-05-14T17:27:13.656Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}