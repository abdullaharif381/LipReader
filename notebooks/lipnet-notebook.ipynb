{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":239068708,"sourceType":"kernelVersion"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 0. Install and Import Dependencies","metadata":{"tags":[],"id":"a3573a47-3689-4668-b62f-5c8451b2b4e9"}},{"cell_type":"code","source":"!pip list","metadata":{"scrolled":true,"tags":[],"id":"ddfbccbe-41ae-4c23-98b1-a13868e2b499","outputId":"3420be4e-63f2-428d-f4d6-073ffa434289","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:18.819584Z","iopub.execute_input":"2025-05-12T07:32:18.820107Z","iopub.status.idle":"2025-05-12T07:32:21.756291Z","shell.execute_reply.started":"2025-05-12T07:32:18.820060Z","shell.execute_reply":"2025-05-12T07:32:21.755391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install opencv-python matplotlib imageio gdown tensorflow","metadata":{"tags":[],"id":"02f907ea-f669-46c7-adcf-7f257e663448","outputId":"9a3240fb-c8a4-43e5-f75c-2289cfe3d91d","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:22.385940Z","iopub.execute_input":"2025-05-12T07:32:22.386242Z","iopub.status.idle":"2025-05-12T07:32:25.724704Z","shell.execute_reply.started":"2025-05-12T07:32:22.386214Z","shell.execute_reply":"2025-05-12T07:32:25.723985Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport tensorflow as tf\nimport numpy as np\nfrom typing import List\nfrom matplotlib import pyplot as plt\nimport imageio","metadata":{"tags":[],"id":"b24af50c-20b8-409d-ad78-30a933fdd669","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:25.725998Z","iopub.execute_input":"2025-05-12T07:32:25.726246Z","iopub.status.idle":"2025-05-12T07:32:41.016577Z","shell.execute_reply.started":"2025-05-12T07:32:25.726225Z","shell.execute_reply":"2025-05-12T07:32:41.015775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.config.list_physical_devices('GPU')","metadata":{"id":"1e3db0b0-e559-4ad6-91fd-e7414b7d75e6","outputId":"29472e0a-7de4-40c1-8131-ad1075e79670","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:41.017866Z","iopub.execute_input":"2025-05-12T07:32:41.018550Z","iopub.status.idle":"2025-05-12T07:32:42.555731Z","shell.execute_reply.started":"2025-05-12T07:32:41.018520Z","shell.execute_reply":"2025-05-12T07:32:42.554962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"physical_devices = tf.config.list_physical_devices('GPU')\ntry:\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\nexcept:\n    pass","metadata":{"tags":[],"id":"378d045a-3003-4f93-b7d2-a25a97774a68","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:42.557908Z","iopub.execute_input":"2025-05-12T07:32:42.558221Z","iopub.status.idle":"2025-05-12T07:32:42.575583Z","shell.execute_reply.started":"2025-05-12T07:32:42.558196Z","shell.execute_reply":"2025-05-12T07:32:42.574963Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Build Data Loading Functions","metadata":{"tags":[],"id":"7a19e88e-c7b9-45c1-ae1e-f2109329c71b"}},{"cell_type":"code","source":"import gdown","metadata":{"tags":[],"id":"8fb99c90-e05a-437f-839d-6e772f8c1dd5","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:42.576298Z","iopub.execute_input":"2025-05-12T07:32:42.576548Z","iopub.status.idle":"2025-05-12T07:32:42.914806Z","shell.execute_reply.started":"2025-05-12T07:32:42.576531Z","shell.execute_reply":"2025-05-12T07:32:42.913998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"url = 'https://drive.google.com/uc?id=1YlvpDLix3S-U8fd-gqRwPcWXAXm8JwjL'\noutput = 'data.zip'\ngdown.download(url, output, quiet=False)\ngdown.extractall('data.zip')","metadata":{"tags":[],"id":"c019e4c6-2af3-4160-99ea-5c8cb009f1a7","outputId":"fdde2fb6-b8cd-4d0a-a74e-b5cf6f7e1e78","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:42.915571Z","iopub.execute_input":"2025-05-12T07:32:42.916024Z","iopub.status.idle":"2025-05-12T07:32:53.384177Z","shell.execute_reply.started":"2025-05-12T07:32:42.916005Z","shell.execute_reply":"2025-05-12T07:32:53.383318Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n****Prompt: Write a Python function using OpenCV and TensorFlow that loads a video from a given file path, extracts all its frames, converts each frame to grayscale,crops a fixed region from each frame (from rows 190 to 236 and columns 80 to 220), and normalizes the resulting frames using the dataset mean and standard deviation. Return the processed frames as a list of normalized tensors.Video loading code ,converting frames into grayscale(25fps)****\n","metadata":{}},{"cell_type":"code","source":"def load_video(path:str) -> List[float]:\n\n    cap = cv2.VideoCapture(path)\n    frames = []\n    for _ in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n        ret, frame = cap.read()\n        frame = tf.image.rgb_to_grayscale(tf.convert_to_tensor(frame))\n        frames.append(frame[190:236,80:220,:])\n    cap.release()\n\n    mean = tf.math.reduce_mean(frames)\n    std = tf.math.reduce_std(tf.cast(frames, tf.float32))\n    return tf.cast((frames - mean), tf.float32) / std","metadata":{"tags":[],"id":"8548cc59-6dfc-4acc-abc3-3e65212db02e","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:53.385087Z","iopub.execute_input":"2025-05-12T07:32:53.385700Z","iopub.status.idle":"2025-05-12T07:32:53.390769Z","shell.execute_reply.started":"2025-05-12T07:32:53.385676Z","shell.execute_reply":"2025-05-12T07:32:53.389934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vocab = [x for x in \"abcdefghijklmnopqrstuvwxyz'?!123456789 \"]","metadata":{"tags":[],"id":"ec735e0b-ec98-4eb0-8f49-c35527d6670a","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:53.391569Z","iopub.execute_input":"2025-05-12T07:32:53.392115Z","iopub.status.idle":"2025-05-12T07:32:53.430691Z","shell.execute_reply.started":"2025-05-12T07:32:53.392049Z","shell.execute_reply":"2025-05-12T07:32:53.430082Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n# List all available physical devices (GPUs)\nphysical_devices = tf.config.list_physical_devices('GPU')\n\n# Check if GPUs are available\nif len(physical_devices) > 0:\n    for gpu in physical_devices:\n        try:\n# Setting memory growth for each GPU to True\n            tf.config.experimental.set_memory_growth(gpu, True)\n        except RuntimeError as e:\n            print(f\"Error setting memory growth on {gpu}: {e}\")\nelse:\n    print(\"No GPU devices found.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:53.431367Z","iopub.execute_input":"2025-05-12T07:32:53.431561Z","iopub.status.idle":"2025-05-12T07:32:53.447852Z","shell.execute_reply.started":"2025-05-12T07:32:53.431547Z","shell.execute_reply":"2025-05-12T07:32:53.447060Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Create two StringLookup layers in tensorFlow.one for converting characters to numbers and another for converting numbers back to characters.Also print the vocabulary and its size.**","metadata":{}},{"cell_type":"code","source":"char_to_num = tf.keras.layers.StringLookup(vocabulary=vocab, oov_token=\"\")\nnum_to_char = tf.keras.layers.StringLookup(\n    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n)\n\nprint(\n    f\"The vocabulary is: {char_to_num.get_vocabulary()} \"\n    f\"(size ={char_to_num.vocabulary_size()})\"\n)","metadata":{"tags":[],"id":"be04e972-d7a5-4a72-82d8-a6bdde1f3ce6","outputId":"d614b1bf-a54b-4d89-8775-48171d1b1550","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:53.450225Z","iopub.execute_input":"2025-05-12T07:32:53.450426Z","iopub.status.idle":"2025-05-12T07:32:53.847597Z","shell.execute_reply.started":"2025-05-12T07:32:53.450412Z","shell.execute_reply":"2025-05-12T07:32:53.846825Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"char_to_num.get_vocabulary()","metadata":{"tags":[],"id":"559f7420-6802-45fa-9ca0-b1ff209b461c","outputId":"1f633218-6900-4c0e-80f8-5176169b10e4","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:53.848269Z","iopub.execute_input":"2025-05-12T07:32:53.848466Z","iopub.status.idle":"2025-05-12T07:32:53.854835Z","shell.execute_reply.started":"2025-05-12T07:32:53.848451Z","shell.execute_reply":"2025-05-12T07:32:53.854163Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#example testing for character_to_num function\nchar_to_num(['n','i','c','k'])","metadata":{"tags":[],"id":"797ff78b-b48f-4e14-bb62-8cd0ebf9501a","outputId":"b2a31191-c321-49ee-86cc-2a4003c26e54","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:53.855689Z","iopub.execute_input":"2025-05-12T07:32:53.855845Z","iopub.status.idle":"2025-05-12T07:32:53.872675Z","shell.execute_reply.started":"2025-05-12T07:32:53.855833Z","shell.execute_reply":"2025-05-12T07:32:53.872111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_to_char([14,9,3,11])","metadata":{"id":"8cd7f4f4-ae77-4509-a4f4-c723787ebad1","outputId":"ad9613de-c145-4d1f-bd92-73f9743794a5","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:53.873399Z","iopub.execute_input":"2025-05-12T07:32:53.873681Z","iopub.status.idle":"2025-05-12T07:32:53.886274Z","shell.execute_reply.started":"2025-05-12T07:32:53.873666Z","shell.execute_reply":"2025-05-12T07:32:53.885546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Returns a list of character token IDs (for use in CTC loss model)\n#sil here is silence\n\n\ndef load_alignments(path:str)->List[str]:\n    with open(path, 'r') as f:\n        lines = f.readlines()\n    tokens = []\n    for line in lines:\n        line = line.split()\n        if line[2] != 'sil':\n            tokens = [*tokens,' ',line[2]]\n    return char_to_num(tf.reshape(tf.strings.unicode_split(tokens,input_encoding='UTF-8'),(-1)))[1:]","metadata":{"tags":[],"id":"9491bab5-6a3c-4f79-879a-8f9fbe73ae2e","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:53.887013Z","iopub.execute_input":"2025-05-12T07:32:53.887539Z","iopub.status.idle":"2025-05-12T07:32:53.899879Z","shell.execute_reply.started":"2025-05-12T07:32:53.887522Z","shell.execute_reply":"2025-05-12T07:32:53.899350Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Write a Python function that takes a TensorFlow tensor representing a file path, decodes it to a string, and extracts the file name. Use this to generate the paths for a video file and its corresponding alignment file. Load the video frames and alignment tokens using appropriate helper functions and return both.**","metadata":{}},{"cell_type":"code","source":"#the following function:Converts a path tensor to a usable string.\n#Extracts the sample name.\n#Loads the corresponding video and phoneme alignment.\n\ndef load_data(path: tf.Tensor):\n    path = path.numpy().decode('utf-8')\n    file_name = os.path.splitext(os.path.basename(path))[0]\n\n    video_path = os.path.join('data', 's1', f'{file_name}.mpg')\n    alignment_path = os.path.join('data', 'alignments', 's1', f'{file_name}.align')\n\n    frames = load_video(video_path)\n    alignments = load_alignments(alignment_path)\n\n    return frames, alignments\n","metadata":{"tags":[],"id":"dd01ca9f-77fb-4643-a2aa-47dd82c5d66b","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:53.900497Z","iopub.execute_input":"2025-05-12T07:32:53.900653Z","iopub.status.idle":"2025-05-12T07:32:53.914729Z","shell.execute_reply.started":"2025-05-12T07:32:53.900642Z","shell.execute_reply":"2025-05-12T07:32:53.914228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_path = '/kaggle/working/data/s1/bbaf2n.mpg'\n","metadata":{"tags":[],"id":"8cb7cc58-31ae-4904-a805-1177a82717d2","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:53.915367Z","iopub.execute_input":"2025-05-12T07:32:53.915606Z","iopub.status.idle":"2025-05-12T07:32:53.928681Z","shell.execute_reply.started":"2025-05-12T07:32:53.915592Z","shell.execute_reply":"2025-05-12T07:32:53.927908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.convert_to_tensor(test_path).numpy().decode('utf-8').split('\\\\')[-1].split('.')[0]","metadata":{"id":"76aa964f-0c84-490d-897a-d00e3966e2c9","outputId":"0b2ae311-c160-45af-872a-cf56568ef100","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:53.929486Z","iopub.execute_input":"2025-05-12T07:32:53.930259Z","iopub.status.idle":"2025-05-12T07:32:53.945309Z","shell.execute_reply.started":"2025-05-12T07:32:53.930235Z","shell.execute_reply":"2025-05-12T07:32:53.944697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"frames, alignments = tf.py_function(load_data, [tf.convert_to_tensor('/kaggle/working/data/s1/bbaf2n.mpg')], (tf.float32, tf.int64))\n","metadata":{"scrolled":true,"tags":[],"id":"eb602c71-8560-4f9e-b26b-08202febb937","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:53.945899Z","iopub.execute_input":"2025-05-12T07:32:53.946128Z","iopub.status.idle":"2025-05-12T07:32:54.379094Z","shell.execute_reply.started":"2025-05-12T07:32:53.946107Z","shell.execute_reply":"2025-05-12T07:32:54.378311Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(frames[40])","metadata":{"id":"0e3184a1-6b02-4b4f-84a8-a0a65f951ea2","outputId":"01d963d8-1f79-46ef-d4c4-622f9059d1ea","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:54.379971Z","iopub.execute_input":"2025-05-12T07:32:54.380263Z","iopub.status.idle":"2025-05-12T07:32:54.614755Z","shell.execute_reply.started":"2025-05-12T07:32:54.380238Z","shell.execute_reply":"2025-05-12T07:32:54.614121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#showing 21 timesteps and the numbers in them are char_to_num representation\nalignments","metadata":{"id":"d7ec0833-d54b-4073-84cf-92d011c60ec1","outputId":"e851ce8d-adf2-414b-8b3d-6c446cbca681","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:54.615534Z","iopub.execute_input":"2025-05-12T07:32:54.615789Z","iopub.status.idle":"2025-05-12T07:32:54.620681Z","shell.execute_reply.started":"2025-05-12T07:32:54.615766Z","shell.execute_reply":"2025-05-12T07:32:54.620129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.strings.reduce_join([bytes.decode(x) for x in num_to_char(alignments.numpy()).numpy()])","metadata":{"id":"fe1ad370-b287-4b46-85a2-7c45b0bd9b10","outputId":"662c2d6c-b7b4-4bef-88f5-9c49bdb46233","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:54.621465Z","iopub.execute_input":"2025-05-12T07:32:54.621659Z","iopub.status.idle":"2025-05-12T07:32:54.639816Z","shell.execute_reply.started":"2025-05-12T07:32:54.621636Z","shell.execute_reply":"2025-05-12T07:32:54.639190Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Create Data Pipeline","metadata":{"tags":[],"id":"c40a7eb4-0c3e-4eab-9291-5611cb68ce08"}},{"cell_type":"code","source":"from matplotlib import pyplot as plt","metadata":{"tags":[],"id":"7686355d-45aa-4c85-ad9c-053e6a9b4d81","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:54.640504Z","iopub.execute_input":"2025-05-12T07:32:54.640699Z","iopub.status.idle":"2025-05-12T07:32:54.652545Z","shell.execute_reply.started":"2025-05-12T07:32:54.640685Z","shell.execute_reply":"2025-05-12T07:32:54.651981Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def mappable_function(path: str):\n    def load_data_wrapper(path: str):  #wrapper function\n        path = bytes.decode(path.numpy())\n        file_name = os.path.basename(path).split('.')[0]\n        video_path = os.path.join('data', 's1', f'{file_name}.mpg')\n        alignment_path = os.path.join('data', 'alignments', 's1', f'{file_name}.align')\n        frames = load_video(video_path)  # Assuming load_video is defined elsewhere\n        alignments = load_alignments(alignment_path)  # Assuming load_alignments is defined elsewhere\n        return frames, alignments\n\n    result = tf.py_function(load_data_wrapper, [path], (tf.float32, tf.int64))\n    return result\n\n#data pipeline setup \ndata = tf.data.Dataset.list_files('./data/s1/*.mpg')\ndata = data.shuffle(500, reshuffle_each_iteration=False)\ndata = data.map(mappable_function)  # Using the wrapper function\ndata = data.padded_batch(2, padded_shapes=([75, None, None, None], [40]))\ndata = data.prefetch(tf.data.AUTOTUNE)# Added for split\ntrain = data.take(450)\ntest = data.skip(450)","metadata":{"tags":[],"id":"f066fea2-91b1-42ed-a67d-00566a1a53ff","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:54.653249Z","iopub.execute_input":"2025-05-12T07:32:54.653448Z","iopub.status.idle":"2025-05-12T07:32:55.733483Z","shell.execute_reply.started":"2025-05-12T07:32:54.653434Z","shell.execute_reply":"2025-05-12T07:32:55.732711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(test)","metadata":{"id":"6b1365bd-7742-41d1-95d4-247021751c3a","outputId":"20f4fa34-bc15-4a92-a11f-a5c8c5a15834","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:55.734292Z","iopub.execute_input":"2025-05-12T07:32:55.734534Z","iopub.status.idle":"2025-05-12T07:32:55.739650Z","shell.execute_reply.started":"2025-05-12T07:32:55.734513Z","shell.execute_reply":"2025-05-12T07:32:55.738917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(frames)","metadata":{"id":"cbebe683-6afd-47fd-bba4-c83b4b13bb32","outputId":"3a5e8b5a-fc63-45e7-bc06-5c8dc9884516","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:55.740427Z","iopub.execute_input":"2025-05-12T07:32:55.740748Z","iopub.status.idle":"2025-05-12T07:32:55.919490Z","shell.execute_reply.started":"2025-05-12T07:32:55.740721Z","shell.execute_reply":"2025-05-12T07:32:55.918927Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample = data.as_numpy_iterator()","metadata":{"tags":[],"id":"5cf2d676-93a9-434c-b3c7-bdcc2577b2e7","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:55.920300Z","iopub.execute_input":"2025-05-12T07:32:55.920563Z","iopub.status.idle":"2025-05-12T07:32:55.978758Z","shell.execute_reply.started":"2025-05-12T07:32:55.920542Z","shell.execute_reply":"2025-05-12T07:32:55.978262Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val = sample.next(); val[0]","metadata":{"scrolled":true,"tags":[],"id":"efa6cd46-7079-46c0-b45b-832f339f6cb0","outputId":"9ff5cb50-5428-4f06-edea-e3be97be4be5","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:55.980085Z","iopub.execute_input":"2025-05-12T07:32:55.980301Z","iopub.status.idle":"2025-05-12T07:32:57.098921Z","shell.execute_reply.started":"2025-05-12T07:32:55.980285Z","shell.execute_reply":"2025-05-12T07:32:57.097288Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 0:videos, 0: 1st video out of the batch,  0: return the first frame in the video\nplt.imshow(val[0][0][35])","metadata":{"tags":[],"id":"c33a87a2-d5e0-4ec9-b174-73ebf41bf03a","outputId":"f490302f-d0da-457c-8e8d-f1d3188a374a","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:57.104157Z","iopub.execute_input":"2025-05-12T07:32:57.104400Z","iopub.status.idle":"2025-05-12T07:32:57.386652Z","shell.execute_reply.started":"2025-05-12T07:32:57.104381Z","shell.execute_reply":"2025-05-12T07:32:57.384289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.strings.reduce_join([num_to_char(word) for word in val[1][0]])","metadata":{"tags":[],"id":"84593332-133c-4205-b7a6-8e235d5e2b3b","outputId":"227ca1c2-959c-420b-a9b0-911beedbb370","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:57.388230Z","iopub.execute_input":"2025-05-12T07:32:57.388570Z","iopub.status.idle":"2025-05-12T07:32:57.483212Z","shell.execute_reply.started":"2025-05-12T07:32:57.388538Z","shell.execute_reply":"2025-05-12T07:32:57.482098Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Design the Deep Neural Network","metadata":{"tags":[],"id":"0f47733c-83bc-465c-b118-b198b492ad37"}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPool3D, Activation, Reshape, SpatialDropout3D, BatchNormalization, TimeDistributed, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler","metadata":{"tags":[],"id":"d8e9a497-191b-4842-afbd-26f5e13c43ba","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:57.486230Z","iopub.execute_input":"2025-05-12T07:32:57.486622Z","iopub.status.idle":"2025-05-12T07:32:57.514949Z","shell.execute_reply.started":"2025-05-12T07:32:57.486592Z","shell.execute_reply":"2025-05-12T07:32:57.513750Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.as_numpy_iterator().next()[0][0].shape","metadata":{"id":"3f753ed2-70b9-4236-8c1c-08ca065dc8bf","outputId":"f6dc678b-8faa-46f5-e5a6-f1b3448c9c61","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:57.516126Z","iopub.execute_input":"2025-05-12T07:32:57.518750Z","iopub.status.idle":"2025-05-12T07:32:58.467710Z","shell.execute_reply.started":"2025-05-12T07:32:57.518730Z","shell.execute_reply":"2025-05-12T07:32:58.467130Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Model creation(Implementing LipNet 2014)\nmodel = Sequential()\nmodel.add(Conv3D(128, 3, input_shape=(75,46,140,1), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPool3D((1,2,2)))\n\nmodel.add(Conv3D(256, 3, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPool3D((1,2,2)))\n\nmodel.add(Conv3D(75, 3, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPool3D((1,2,2)))\n\nmodel.add(Reshape((75, 5 * 17 * 75)))\n\nmodel.add(Bidirectional(LSTM(128, kernel_initializer='Orthogonal', return_sequences=True)))\nmodel.add(Dropout(.5))\n\nmodel.add(Bidirectional(LSTM(128, kernel_initializer='Orthogonal', return_sequences=True)))\nmodel.add(Dropout(.5))\n\nmodel.add(Dense(char_to_num.vocabulary_size()+1, kernel_initializer='he_normal', activation='softmax'))","metadata":{"tags":[],"id":"f9171056-a352-491a-9ed9-92b28ced268e","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:32:58.468392Z","iopub.execute_input":"2025-05-12T07:32:58.468633Z","iopub.status.idle":"2025-05-12T07:33:00.503136Z","shell.execute_reply.started":"2025-05-12T07:32:58.468609Z","shell.execute_reply":"2025-05-12T07:33:00.502580Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"tags":[],"id":"78851825-2bcd-42a9-b7f2-28bb5a6bf43a","outputId":"0a59976a-b771-412e-9ab0-4884e920d280","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:33:00.503690Z","iopub.execute_input":"2025-05-12T07:33:00.503863Z","iopub.status.idle":"2025-05-12T07:33:00.525373Z","shell.execute_reply.started":"2025-05-12T07:33:00.503850Z","shell.execute_reply":"2025-05-12T07:33:00.524684Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Setup Training Options and Train","metadata":{"tags":[],"id":"2ec02176-5c26-46c3-aff7-8352e6563c7d"}},{"cell_type":"code","source":"def scheduler(epoch,lr):\n    if epoch < 30:\n        return lr\n    else:\n        return lr * tf.math.exp(-0.1)","metadata":{"tags":[],"id":"ab015fd0-7fb4-4d5d-9fa2-30a05dbd515a","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:33:06.934407Z","iopub.execute_input":"2025-05-12T07:33:06.935102Z","iopub.status.idle":"2025-05-12T07:33:06.938724Z","shell.execute_reply.started":"2025-05-12T07:33:06.935050Z","shell.execute_reply":"2025-05-12T07:33:06.937925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Defining a loss function for LipNet(CTCLoss used in the paper)\ndef CTCLoss(y_true, y_pred):\n    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n\n    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n\n    loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n    return loss","metadata":{"tags":[],"id":"c564d5c9-db54-4e88-b311-9aeab7fb3e69","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:33:07.314970Z","iopub.execute_input":"2025-05-12T07:33:07.315715Z","iopub.status.idle":"2025-05-12T07:33:07.320399Z","shell.execute_reply.started":"2025-05-12T07:33:07.315687Z","shell.execute_reply":"2025-05-12T07:33:07.319655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#This function tests a sample video after every epoch\nclass ProduceExample(tf.keras.callbacks.Callback):\n    def __init__(self, dataset) -> None:\n        self.dataset = dataset.as_numpy_iterator()\n\n    def on_epoch_end(self, epoch, logs=None) -> None:\n        data = self.dataset.next()\n        yhat = self.model.predict(data[0])\n        decoded = tf.keras.backend.ctc_decode(yhat, [75,75], greedy=False)[0][0].numpy()\n        for x in range(len(yhat)):\n            print('Original:', tf.strings.reduce_join(num_to_char(data[1][x])).numpy().decode('utf-8'))\n            print('Prediction:', tf.strings.reduce_join(num_to_char(decoded[x])).numpy().decode('utf-8'))\n            print('~'*100)","metadata":{"tags":[],"id":"a26dc3fc-a19c-4378-bd8c-e2b597a1d15c","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:33:08.944390Z","iopub.execute_input":"2025-05-12T07:33:08.945011Z","iopub.status.idle":"2025-05-12T07:33:08.950537Z","shell.execute_reply.started":"2025-05-12T07:33:08.944987Z","shell.execute_reply":"2025-05-12T07:33:08.949653Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(optimizer=Adam(learning_rate=0.0001), loss=CTCLoss)","metadata":{"tags":[],"id":"04be90d8-2482-46f9-b513-d5f4f8001c7e","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:33:11.978910Z","iopub.execute_input":"2025-05-12T07:33:11.979454Z","iopub.status.idle":"2025-05-12T07:33:11.992714Z","shell.execute_reply.started":"2025-05-12T07:33:11.979430Z","shell.execute_reply":"2025-05-12T07:33:11.992006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n# Create a directory to save checkpoints\ncheckpoint_dir = './checkpoints'\nos.makedirs(checkpoint_dir, exist_ok=True)\n\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=os.path.join(checkpoint_dir, 'model_epoch_{epoch:02d}.keras'),\n    save_weights_only=False,\n    save_best_only=False,\n    save_freq='epoch',\n    verbose=1\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:33:15.640940Z","iopub.execute_input":"2025-05-12T07:33:15.641533Z","iopub.status.idle":"2025-05-12T07:33:15.646669Z","shell.execute_reply.started":"2025-05-12T07:33:15.641507Z","shell.execute_reply":"2025-05-12T07:33:15.645796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\ntry:\n    latest_model_path = '/kaggle/input/notebookf0d2dc04cb/checkpoints/'\n    model = tf.saved_model.load(latest_model_path)\n    print(f\"Successfully loaded model using tf.saved_model.load() from: {latest_model_path}\")\n\n    # You might need to wrap the loaded SavedModel in a Keras layer\n    # if you want to use .fit() directly. This depends on how it was saved.\n    # For example:z\n    # loaded_keras_model = tf.keras.models.Sequential([\n    #     tf.keras.layers.Input(shape=input_shape), # Replace input_shape\n    #     model.signatures['serving_default'] # Access the inference function\n    # ])\n    #\n    # However, for resuming training, it's usually better if it's a direct Keras model.\n\nexcept Exception as e:\n    print(f\"Error loading with tf.saved_model.load(): {e}\")\n    print(\"Trying the next method...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:35:10.234002Z","iopub.execute_input":"2025-05-12T07:35:10.234339Z","iopub.status.idle":"2025-05-12T07:35:10.244090Z","shell.execute_reply.started":"2025-05-12T07:35:10.234315Z","shell.execute_reply":"2025-05-12T07:35:10.243486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the previously saved model\nlatest_checkpoint_path = '/kaggle/input/notebookf0d2dc04cb/checkpoints/model_epoch_66.keras'\nmodel = tf.keras.models.load_model(latest_checkpoint_path, custom_objects={'CTCLoss': CTCLoss})\nprint(f\"Resuming training from epoch {int(latest_checkpoint_path.split('_')[-1].split('.')[0])}\")\n\ninitial_epoch = int(latest_checkpoint_path.split('_')[-1].split('.')[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:39:52.169954Z","iopub.execute_input":"2025-05-12T07:39:52.170676Z","iopub.status.idle":"2025-05-12T07:39:54.252173Z","shell.execute_reply.started":"2025-05-12T07:39:52.170651Z","shell.execute_reply":"2025-05-12T07:39:54.251493Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.fit(\n\n    train,\n\n    validation_data=test,\n\n    epochs=100,\n\n    callbacks=[checkpoint_callback],\n\n    initial_epoch=initial_epoch\n\n)\nmodel.save(\"final_model.keras\")","metadata":{"tags":[],"id":"8ffba483-aa61-4bbe-a15f-a73e1ddf097c","outputId":"e1541f66-d065-4a07-81e8-3c460fecc3f9","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:38:56.637226Z","iopub.execute_input":"2025-05-12T07:38:56.637895Z","iopub.status.idle":"2025-05-12T07:39:10.822338Z","shell.execute_reply.started":"2025-05-12T07:38:56.637872Z","shell.execute_reply":"2025-05-12T07:39:10.821114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"lipnet.keras\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}